{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import cv2\n",
    "import NMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from dataset import SeismicDataset, NormalMoveout\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = SeismicDataset(seismo_dir='./train/raw/', \n",
    "                         velocity_dir='./train/outputs/', \n",
    "                         transform=transforms.Compose([ToTensor()]))\n",
    "val_dataset = SeismicDataset(seismo_dir='./val/raw/', \n",
    "                         velocity_dir='./val/outputs/', \n",
    "                         transform=transforms.Compose([ToTensor()]))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean and std calculation for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seismogram_stack = train_dataset[0]['seismogram'][None]\n",
    "velocity_stack = torch.FloatTensor(train_dataset[0]['velocity'][None])\n",
    "\n",
    "for i in range(1, len(train_dataset)):\n",
    "    _el = train_dataset[i]\n",
    "    seismogram_stack = torch.cat([seismogram_stack, (_el['seismogram'][None])], dim=0)\n",
    "    velocity_stack = torch.cat([velocity_stack, torch.FloatTensor(_el['velocity'][None])], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seismo_mean = torch.mean(seismogram_stack).to(device)\n",
    "seismo_std = torch.std(seismogram_stack).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_mean = torch.mean(velocity_stack).to(device)\n",
    "velocity_std = torch.std(velocity_stack).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validation procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, writer=None, global_step=None, name=None, normalize=True):\n",
    "    model.train()\n",
    "    train_losses = AverageMeter()\n",
    "    \n",
    "    for idx, batch in enumerate((loader)):\n",
    "        x = torch.FloatTensor(batch['seismogram']).to(device)\n",
    "        y = torch.FloatTensor(batch['velocity']).to(device)\n",
    "        \n",
    "        if normalize:\n",
    "            x = (x - seismo_mean) / seismo_std\n",
    "        else:\n",
    "            x = torch.log(torch.abs(x)) # torch.log(x)\n",
    "            x[1 -torch.isfinite(x)] = 0.0\n",
    "        y = (y - velocity_mean) / velocity_std\n",
    "            \n",
    "        \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        loss = criterion(y, y_pred)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.update(loss.item(), x.size(0))\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(f\"{name}/train_loss.avg\", train_losses.avg, global_step=global_step + idx)\n",
    "            \n",
    "    return train_losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, criterion, writer=None, global_step=None, name=None, normalize=True):\n",
    "    model.eval()\n",
    "    validate_losses = AverageMeter()\n",
    "    \n",
    "    for idx, batch in enumerate((loader)):\n",
    "        x = torch.FloatTensor(batch['seismogram']).to(device)\n",
    "        y = torch.FloatTensor(batch['velocity']).to(device)\n",
    "        \n",
    "        if normalize:\n",
    "            x = (x - seismo_mean) / seismo_std\n",
    "        else:\n",
    "            x = torch.log(torch.abs(x)) # torch.log(x)\n",
    "            x[1 -torch.isfinite(x)] = 0.0\n",
    "        y = (y - velocity_mean) / velocity_std\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        loss = criterion(y, y_pred)\n",
    "        validate_losses.update(loss.item(), x.size(0))\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(f\"{name}/val_loss.avg\", validate_losses.avg, global_step=global_step + idx)\n",
    "    return validate_losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import AccelerationPredictor, AccelerationPredictor2\n",
    "\n",
    "model = AccelerationPredictor().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "\n",
    "criterion = nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir_name = 'training_logs/1'\n",
    "save_model_each = 10\n",
    "normalize = True\n",
    "num_epoch = 100\n",
    "\n",
    "writer = SummaryWriter(log_dir=os.path.join(experiment_dir_name, 'logs'))\n",
    "os.makedirs(experiment_dir_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(num_epoch)):\n",
    "    \n",
    "    train_loss = train(model=model, loader=train_loader, optimizer=optimizer, criterion=criterion,\n",
    "                       writer=writer, global_step=len(train_loader.dataset) * epoch,\n",
    "                       name=f\"{experiment_dir_name}_by_batch\", normalize=normalize)\n",
    "    \n",
    "    val_loss = validate(model=model, loader=val_loader, criterion=criterion,\n",
    "                        writer=writer, global_step=len(train_loader.dataset) * epoch,\n",
    "                        name=f\"{experiment_dir_name}_by_batch\", normalize=normalize)\n",
    "    \n",
    "    model_name = f\"emd_loss_epoch_{epoch}_train_{train_loss}_{val_loss}.pth\"\n",
    "    \n",
    "    if epoch % save_model_each == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(experiment_dir_name, model_name))\n",
    "        \n",
    "    writer.add_scalar(f\"{experiment_dir_name}_by_epoch/train_loss\", train_loss, global_step=epoch)\n",
    "    writer.add_scalar(f\"{experiment_dir_name}_by_epoch/val_loss\", val_loss, global_step=epoch)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    print(\"Epoch: {}, Train: {}, Val: {}\".format(epoch, train_loss, val_loss))\n",
    "\n",
    "writer.export_scalars_to_json(os.path.join(experiment_dir_name, 'all_scalars.json'))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
